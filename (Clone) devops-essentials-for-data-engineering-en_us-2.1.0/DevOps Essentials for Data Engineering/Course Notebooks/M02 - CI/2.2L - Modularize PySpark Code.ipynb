{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1daad7e-7ada-416e-9cc2-1f8847137514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "665e4254-1f59-477b-96fa-b8f4e16f976b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.2 Lab - Modularize PySpark Code\n",
    "\n",
    "### Estimated Duration: 15-20 minutes\n",
    "\n",
    "By the end of this lab, you will practice analyzing a PySpark script by breaking it down into smaller, reusable functions, and assessing how well their changes improve the code's clarity and ease of maintenance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83dd0997-a909-4209-9394-36f462555eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1ad302e-774c-4b3e-bc6c-61de02aacc16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. \n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course.\n",
    "\n",
    "##### The notebook \"2.1 - Modularizing PySpark Code - Required\" sets up the catalogs for this course. If you have not run this notebook, the catalogs will not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04a3cdaf-107d-403b-9505-f85c1cf3d622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.2L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d504c4c-875a-4eac-8a3f-958623db834d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to view your current default catalog and schema. \n",
    "\n",
    "  Confirm the following:\n",
    "- The default catalog is your unique catalog name (shown above).\n",
    "- The current schema is **default**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9488afb0-5c53-4cfe-96a7-fff6639f7eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "266927c6-bc5b-4e2a-b85a-a93a8f73507b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Review the Provided PySpark Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efcbf760-be28-4e29-b0b2-d3ccc4c56e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to preview the **samples.nyctaxi.trips** table. Confirm the table exists and view the data.\n",
    "\n",
    "    Notice the following:\n",
    "    - All columns are in lower case\n",
    "    - **trip_distance** is currently in miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdb09567-7820-4497-8568-79285515c7df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM samples.nyctaxi.trips \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09a409da-7194-41f4-b54e-d71da9276532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. You have been provided with the following PySpark script that performs the following:\n",
    "\n",
    "   a. Reads from the **samples.nyctaxi.trips** table.\n",
    "\n",
    "   b. Creates a new column named **trip_distance_km** that converts **trip_distance** to kilometers and rounds it to two decimal places.\n",
    "\n",
    "   c. Converts all of the column names to uppercase.\n",
    "\n",
    "   d. Saves the DataFrame as a table named **nyc_lab_solution_table** in your specific catalog (`DA.catalog_name`).\n",
    "\n",
    "   Run the cell below and confirm that the **nyc_lab_solution_table** table was created with all uppercase column names and the new **trip_distance_km** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f02e37b-fe2f-4ce1-9a4d-fed7697f0047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Run the code to view the default catalog the table is being written to.\n",
    "print(DA.catalog_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b227e9f6-cca5-4d7f-81c0-e82fd489ef76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the data and create a new column named trip_distance_km\n",
    "new_taxi = (spark\n",
    "            .read\n",
    "            .table(\"samples.nyctaxi.trips\")\n",
    "            .withColumn(\"trip_distance_km\", F.round(F.col(\"trip_distance\") * 1.60934, 2))\n",
    "        )\n",
    "\n",
    "\n",
    "## Upper case all columns\n",
    "new_taxi = new_taxi.select([F.col(col).alias(col.upper()) for col in new_taxi.columns])\n",
    "\n",
    "\n",
    "## Save the table to the your catalog\n",
    "(new_taxi\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .saveAsTable(f'{DA.catalog_name}.default.nyc_lab_solution_table')\n",
    ")\n",
    "\n",
    "## View the final table\n",
    "display(spark.table(f'{DA.catalog_name}.default.nyc_lab_solution_table'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6d7e6c7-ead8-4bed-bfda-97ca796b17ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Modularize the PySpark Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "411bfb8c-abfe-46f4-9452-16c1086a2021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Your task is to take the provided Spark code from above and break it down into modular functions. Each function should perform a specific part of the task, making it easier to test, reuse, and maintain.\n",
    "\n",
    "    There are a variety of ways to solve this problem. For consistency in this example, create the following functions:\n",
    "\n",
    "    - `convert_miles_to_km`: Converts a column from miles to kilometers and rounds the result to two decimal places.\n",
    "\n",
    "    - `uppercase_column_names`: Converts all column names in the DataFrame to uppercase.\n",
    "\n",
    "    - `load_data`: Reads the table.\n",
    "\n",
    "    - `save_to_catalog`: Saves the DataFrame as a new table in your catalog.\n",
    "\n",
    "**NOTE:** The `load_data` and `save_to_catalog` functions have already been created for you. \n",
    "\n",
    "**TO DO:** Create the `convert_miles_to_km` and `uppercase_column_names` in the cell below.\n",
    "\n",
    "**HINT:** The solution functions can be found in **[./src_lab/lab_functions/transforms.py]($./src_lab/lab_functions/transforms.py)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "687ec091-34ea-447a-9873-5769c296fc77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "## load_data has already been created for you\n",
    "def load_data(table_name):\n",
    "    return spark.read.table(table_name)\n",
    "\n",
    "\n",
    "## save_to_catalog has been created for you\n",
    "def save_to_catalog(df, catalog_name, schema_name, table_name):\n",
    "    (df\n",
    "     .write\n",
    "     .mode('overwrite')\n",
    "     .saveAsTable(f'{catalog_name}.default.{table_name}')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1333e9f-77e5-4fa3-9646-7485f1e23c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "## load_data has already been created for you\n",
    "def load_data(table_name):\n",
    "    return spark.read.table(table_name)\n",
    "\n",
    "\n",
    "## save_to_catalog has been created for you\n",
    "def save_to_catalog(df, catalog_name, schema_name, table_name):\n",
    "    (df\n",
    "     .write\n",
    "     .mode('overwrite')\n",
    "     .saveAsTable(f'{catalog_name}.{schema_name}.{table_name}')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2af00b71-3796-431e-99a4-85930d222a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## convert_miles_to_km\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dbe4698-b6f8-4cdb-9a8c-260e1cb03e29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## convert_miles_to_km\n",
    "def convert_miles_to_km(df, new_column_name, miles_column):\n",
    "    return df.withColumn(new_column_name, F.round(F.col(miles_column) * 1.60934, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8ab095a-59fa-43ed-ba79-f7ecea418a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## uppercase_column_names\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2448f40b-20fc-44b0-a5f3-b58f73cda7cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## uppercase_column_names\n",
    "def uppercase_columns_names(df):\n",
    "    return df.select([F.col(col).alias(col.upper()) for col in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38a797fa-fdcd-4f5f-b3cb-dfecedd63692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run your functions to obtain the same results as the original PySpark code. The `save_to_catalog` function will name your new table **my_lab_table**. \n",
    "\n",
    "**NOTE:** If you are receiving a schema mismatch error that is because you are trying to overwrite a table you created with a different schema. Delete the table and recreate the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c633681-f22c-4e0f-8a43-328d5db2fa40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Load table\n",
    "df = load_data(\"samples.nyctaxi.trips\")\n",
    "\n",
    "## Convert miles to km\n",
    "## TODO - Add your function here\n",
    "\n",
    "## Upcase column\n",
    "## TODO - Add your function here\n",
    "\n",
    "## Save DataFrame as a table in your catalog\n",
    "save_to_catalog(df, catalog_name = DA.catalog_name, schema_name=\"default\", table_name = \"my_lab_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98f1cf4a-4bc1-41e2-9171-6c8641c5f9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Load table\n",
    "df = load_data(\"samples.nyctaxi.trips\")\n",
    "\n",
    "## Convert miles to km\n",
    "df = convert_miles_to_km(df, new_column_name = \"trip_distance_km\", miles_column = \"trip_distance\")\n",
    "\n",
    "## Upcase column\n",
    "df = uppercase_columns_names(df)\n",
    "\n",
    "## Save DataFrame as a table in your catalog\n",
    "save_to_catalog(df, catalog_name = DA.catalog_name, schema_name=\"default\", table_name = \"my_lab_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "316968fd-6625-4dc1-aec1-d8f2941aef46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the following cell to test that the original table created in cell 11 (**nyc_km_solution_table**) is the same as your new table created by the functions above (**my_lab_table**). The test uses the PySpark `assertDataFrameEqual` method.\n",
    "\n",
    "    If there is an error, it means the original table is not the same as your new table, and you need to fix your functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f775292b-3e81-432d-b832-e1df13bda719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "\n",
    "# Read the tables (solution and your created table)\n",
    "solution_df = spark.read.table(f\"{DA.catalog_name}.default.nyc_lab_solution_table\")\n",
    "user_df = spark.read.table(f\"{DA.catalog_name}.default.my_lab_table\")\n",
    "\n",
    "# Use assertDataFrameEqual to compare the two tables. Return an error if the tables are different.\n",
    "assertDataFrameEqual(solution_df, user_df)\n",
    "\n",
    "print(\"The tables are identical! Functions were created correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e828968-6dc3-4953-bbac-3ca29d0d054f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.2L - Modularize PySpark Code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
